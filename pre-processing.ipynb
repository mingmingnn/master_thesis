{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0baebfe8",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df = pd.read_excel('C:/Users/ming/Desktop/master project/Copy of Data_All_JAN_2024_Anonymized__For_Analysis_V1.2.xlsx', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual column name\n",
    "print(df.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed Columns\n",
    "selected_columns = [\n",
    "    # Demographic Data\n",
    "    'Style', 'Platform', 'Earpiece Configuration', \n",
    "    'What is your current age?', 'What is your gender?', \n",
    "    'How long have you been using hearing aids?', \n",
    "    'How long have you been using your current hearing aids?', \n",
    "\n",
    "    # Target Variable \n",
    "    'Satisfaction', 'Average comfort score',  \n",
    "\n",
    "    # Structured Scoring\n",
    "    'Slip Out', 'Annoying', 'Change Position', \n",
    "    'Too tight', 'Itchiness', 'Soreness', 'Take off hearing aids', \n",
    "    'Painful',  \n",
    "\n",
    "    # Open-ended Responses\n",
    "    'Please describe the discomfort that you are experiencing', \n",
    "    'What do you believe is causing this discomfort?', \n",
    "    'What do you believe can be done to improve the wearing comfort for your hearing aids?'\n",
    "]\n",
    "\n",
    "open_ended_columns = [\n",
    "    'Please describe the discomfort that you are experiencing', \n",
    "    'What do you believe is causing this discomfort?', \n",
    "    'What do you believe can be done to improve the wearing comfort for your hearing aids?'\n",
    "]\n",
    "\n",
    "df = df[selected_columns]\n",
    "# print(df.head())  \n",
    "\n",
    "print(df.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep open-eneded responses that are not null\n",
    "# Keep rows with at least one non-null response\n",
    "filtered_df1 = df.dropna(how='all', subset=open_ended_columns) \n",
    "filtered_df1 = filtered_df1[(filtered_df1[open_ended_columns].applymap(lambda x: isinstance(x, str) and x.strip() != '')).any(axis=1)]\n",
    "\n",
    "print(filtered_df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfdf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "def get_roberta_sentiment(text):\n",
    "    neutral_texts = [\"none\", \"n/a\", \"-\", \"no issues\", \"no problem\", \"no discomfort\", \"no pain\"]\n",
    "    negative_keywords = [\"itchy\", \"pain\", \"sore\", \"uncomfortable\", \"pressure\", \"tight\"]\n",
    "    text = str(text).strip().lower()\n",
    "\n",
    "    if text in neutral_texts:\n",
    "        return 0  \n",
    "\n",
    "    if any(word in text for word in negative_keywords):\n",
    "        return -0.5 \n",
    "\n",
    "    result = sentiment_model(text[:512])\n",
    "    label = result[0]['label']  # 'LABEL_0' (Negative), 'LABEL_1' (Neutral), 'LABEL_2' (Positive)\n",
    "    score = result[0]['score'] \n",
    "\n",
    "    if score < 0.6:\n",
    "        return 0  \n",
    "\n",
    "    if label == 'LABEL_2':  \n",
    "        return score\n",
    "    elif label == 'LABEL_0':  \n",
    "        return -score\n",
    "    else:  \n",
    "        return 0\n",
    "\n",
    "df['BERT_Sentiment_Score'] = df[text_column].apply(get_roberta_sentiment)\n",
    "df['Sentiment_Label'] = df[text_column].apply(lambda x: sentiment_model(str(x)[:512])[0]['label'])\n",
    "\n",
    "# Statistical Distribution of Categories\n",
    "label_counts = df['Sentiment_Label'].value_counts()\n",
    "print(\"RoBERTa predicted sentiment category distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Visualization of Category Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.xlabel(\"Sentiment Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Sentiment Labels (RoBERTa)\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization of Sentiment Score Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['BERT_Sentiment_Score'], bins=30, kde=True)\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Improved RoBERTa Sentiment Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd584c4",
   "metadata": {},
   "source": [
    "# Train a model based on RoBERTa-base..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('C:/Users/ming/Desktop/master project/200_text_label.xlsx')\n",
    "df = df[['Text', 'label']].dropna().reset_index(drop=True)\n",
    "df[\"Text\"] = df[\"Text\"].astype(str)\n",
    "\n",
    "# Convert label to start from 0\n",
    "label_map = {-1: 0, 0: 1, 1: 2} \n",
    "df[\"label\"] = df[\"label\"].map(label_map).astype(int)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Text\"], df[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "val_encodings = tokenize_function(val_texts.tolist())\n",
    "\n",
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)  \n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = SentimentDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3) \n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,  \n",
    "    per_device_train_batch_size=8,  \n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=50,  \n",
    "    weight_decay=0.01,  \n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"sentiment-roberta\")\n",
    "\n",
    "# Map back to the original label during prediction\n",
    "inverse_label_map = {0: -1, 1: 0, 2: 1}  \n",
    "\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    model.to(\"cpu\") \n",
    "    inputs = {key: val.to(\"cpu\") for key, val in inputs.items()}  \n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return inverse_label_map[pred] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(predict_sentiment(\"I love this product!\"))  # Expected 1\n",
    "print(predict_sentiment(\"This is terrible.\"))  # Expected -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-2\n",
    "print(predict_sentiment(\"No discomfort at all\"))   # Expected 1\n",
    "print(predict_sentiment(\"Very tight, uncomfortable\"))   # Expected -1\n",
    "print(predict_sentiment(\"It's okay\"))  # Expected 0\n",
    "print(predict_sentiment(\"terrible sound.\"))  # Expected -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ea740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for example in val_texts.tolist(): \n",
    "    true_labels.append(inverse_label_map[val_labels.tolist()[val_texts.tolist().index(example)]])\n",
    "    pred_labels.append(predict_sentiment(example))\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", classification_report(true_labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ad1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "trainer.model.to(device)\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "eval_accuracy = eval_results[\"eval_accuracy\"]\n",
    "\n",
    "print(f\"Eval Loss: {eval_loss}\")\n",
    "print(f\"Eval Accuracy: {eval_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save the original RoBERTa tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer.save_pretrained(\"sentiment-roberta\")\n",
    "\n",
    "print(\"Tokenizer has been saved to sentiment-roberta.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200736f",
   "metadata": {},
   "source": [
    "# Satisfaction & Comfort Score Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['Sentiment_Label_Num'] = df_encoded['Custom_Sentiment_Label'].map({\n",
    "    'LABEL_0': -1,  # Negative\n",
    "    'LABEL_1': 0,   # Neutral\n",
    "    'LABEL_2': 1    # Positive\n",
    "})\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=['Custom_Sentiment_Label'])\n",
    "\n",
    "for col in open_ended_columns:\n",
    "    df_encoded[f'{col}_length'] = df_encoded[col].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc39a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "open_ended_columns = [\n",
    "    'Please describe the discomfort that you are experiencing', \n",
    "    'What do you believe is causing this discomfort?', \n",
    "    'What do you believe can be done to improve the wearing comfort for your hearing aids?'\n",
    "]\n",
    "\n",
    "target_comfort = 'Average comfort score'\n",
    "target_satisfaction = 'Satisfaction'\n",
    "features = df_encoded.drop(columns=[target_comfort, target_satisfaction] + open_ended_columns)\n",
    "length_cols = [col for col in features.columns if col.endswith('_length')]\n",
    "features = features.drop(columns=length_cols)\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(features, df_encoded[target_comfort], test_size=0.3, random_state=42)\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(features, df_encoded[target_satisfaction], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train a comfort prediction model\n",
    "model_comfort = RandomForestRegressor(random_state=42)\n",
    "model_comfort.fit(X_train_c, y_train_c)\n",
    "predictions_c = model_comfort.predict(X_test_c)\n",
    "\n",
    "# Train a satisfaction prediction model\n",
    "model_satisfaction = RandomForestRegressor(random_state=42)\n",
    "model_satisfaction.fit(X_train_s, y_train_s)\n",
    "predictions_s = model_satisfaction.predict(X_test_s)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Comfort Score Model - MSE:\", mean_squared_error(y_test_c, predictions_c))\n",
    "print(\"Comfort Score Model - R²:\", r2_score(y_test_c, predictions_c))\n",
    "print(\"Satisfaction Score Model - MSE:\", mean_squared_error(y_test_s, predictions_s))\n",
    "print(\"Satisfaction Score Model - R²:\", r2_score(y_test_s, predictions_s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a26384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comfort prediction results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test_c, predictions_c, alpha=0.6, color='skyblue')\n",
    "plt.plot([min(y_test_c), max(y_test_c)], [min(y_test_c), max(y_test_c)], color='red', linestyle='--')\n",
    "plt.title('Comfort Score: True vs Predicted')\n",
    "plt.xlabel('True Comfort Score')\n",
    "plt.ylabel('Predicted Comfort Score')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Satisfaction prediction results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test_s, predictions_s, alpha=0.6, color='lightcoral')\n",
    "plt.plot([min(y_test_s), max(y_test_s)], [min(y_test_s), max(y_test_s)], color='red', linestyle='--')\n",
    "plt.title('Satisfaction Score: True vs Predicted')\n",
    "plt.xlabel('True Satisfaction Score')\n",
    "plt.ylabel('Predicted Satisfaction Score')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "importance_comfort_df = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': importance_comfort\n",
    "})\n",
    "\n",
    "# Top 10 features for comfort score\n",
    "importance_comfort_df = importance_comfort_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "print(\"Top 10 features for comfort score:\")\n",
    "print(importance_comfort_df)\n",
    "\n",
    "# Figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_comfort_df['Feature'], importance_comfort_df['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.title('Top 10 Feature Importance for Comfort Score')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90da554",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_satisfaction_df = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': importance_satisfaction\n",
    "})\n",
    "\n",
    "# Top 20 features for satisfaction score\n",
    "importance_satisfaction_df = importance_satisfaction_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "print(\"Top 20 features for satisfaction score:\")\n",
    "print(importance_satisfaction_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance_satisfaction_df['Feature'], importance_satisfaction_df['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis() \n",
    "plt.title('Top 20 Feature Importance for Satisfaction Score')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train the Comfort Score Prediction Model with Best Parameters\n",
    "\n",
    "best_model_comfort = RandomForestRegressor(\n",
    "    max_depth=15,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=300,\n",
    "    random_state=42\n",
    ")\n",
    "best_model_comfort.fit(X_train_c, y_train_c)\n",
    "best_predictions_c = best_model_comfort.predict(X_test_c)\n",
    "\n",
    "print(\"Best parameters (Comfort score prediction): - MSE:\", mean_squared_error(y_test_c, best_predictions_c))\n",
    "print(\"Best parameters (Comfort score prediction): - R²:\", r2_score(y_test_c, best_predictions_c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc488ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results of the two prediction models (comfort and satisfaction) ---- stacking model\n",
    "# in 2 model\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "stacked_features = np.column_stack((pred_xgb, predictions_s))\n",
    "stacked_targets = np.column_stack((y_test_c, y_test_s))\n",
    "\n",
    "stacked_model = MultiOutputRegressor(LinearRegression())\n",
    "stacked_model.fit(stacked_features, stacked_targets)\n",
    "\n",
    "stacked_pred = stacked_model.predict(stacked_features)\n",
    "\n",
    "print(\"Stacking Model (Comfort) - MSE:\", mean_squared_error(y_test_c, stacked_pred[:, 0]))\n",
    "print(\"Stacking Model (Comfort) - R²:\", r2_score(y_test_c, stacked_pred[:, 0]))\n",
    "\n",
    "print(\"Stacking Model (Satisfaction) - MSE:\", mean_squared_error(y_test_s, stacked_pred[:, 1]))\n",
    "print(\"Stacking Model (Satisfaction) - R²:\", r2_score(y_test_s, stacked_pred[:, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce to 20 principal components.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "X_train_pca = pca.fit_transform(X_train_c)\n",
    "X_test_pca = pca.transform(X_test_c)\n",
    "print(\"Feature dimensions after PCA reduction\", X_train_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3890e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAP-based feature importance\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model_satisfaction)\n",
    "shap_values = explainer.shap_values(X_test_s)\n",
    "shap.summary_plot(shap_values, X_test_s, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "residuals = y_test_s - predictions_s\n",
    "sns.histplot(residuals, bins=30, kde=True, color='lightcoral')\n",
    "plt.title('Residual Distribution for Satisfaction Score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
